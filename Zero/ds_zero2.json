{
    "_comment": "ZeRO Stage 2: Shard OPTIMIZER + GRADIENTS. Each GPU holds full params, but grads are reduce-scattered and optimizer states are partitioned.",

    "train_batch_size": 16,
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 2,

    "zero_optimization": {
        "stage": 2,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "allgather_bucket_size": 5e8,
        "contiguous_gradients": true
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-5,
            "betas": [0.9, 0.95],
            "eps": 1e-8,
            "weight_decay": 0.01
        }
    },

    "bf16": {
        "enabled": true
    },

    "gradient_clipping": 1.0,

    "steps_per_print": 9999999,
    "wall_clock_breakdown": true
}
