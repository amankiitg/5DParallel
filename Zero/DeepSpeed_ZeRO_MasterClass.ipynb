{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeed ZeRO: A Complete Guide to Sharded Data Parallelism\n",
    "\n",
    "## Fine-tuning a 7B Language Model with ZeRO Stages 0\u20133\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a deep-dive into **DeepSpeed ZeRO** (Zero Redundancy Optimizer), the most widely used\n",
    "technique for training large language models when they don't fit in a single GPU's memory.\n",
    "\n",
    "We will:\n",
    "1. Understand **why** ZeRO exists \u2014 the memory problem in distributed training\n",
    "2. Learn **how** each ZeRO stage works \u2014 what gets sharded and why\n",
    "3. Train a **real 7B model** (Pythia-6.9B) on instruction-following data (Alpaca)\n",
    "4. Compare ZeRO stages 0\u21923 on **memory, throughput, and convergence**\n",
    "5. Run **inference** with the fine-tuned model to verify quality\n",
    "\n",
    "### Hardware\n",
    "- **Recommended:** 2\u00d7 NVIDIA B200 (192 GB each) or 2\u00d7 H100/A100 80GB\n",
    "- **Minimum:** 2\u00d7 A100 40GB (use Pythia-2.8B instead of 6.9B)\n",
    "\n",
    "### RunPod Configuration\n",
    "```\n",
    "GPU Pod: 2\u00d7 B200 192GB  (or 2\u00d7 H100 80GB, or 2\u00d7 A100 80GB)\n",
    "Template: RunPod PyTorch 2.1+\n",
    "Disk: 100 GB (model weights + checkpoints)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: The Memory Problem\n",
    "\n",
    "## Where Does GPU Memory Go During Training?\n",
    "\n",
    "When you train a model with Adam optimizer in mixed precision (bf16), each parameter consumes:\n",
    "\n",
    "| Component | Bytes per Parameter | For 7B Model | For 70B Model |\n",
    "|-----------|-------------------|-------------|---------------|\n",
    "| Model parameters (bf16) | 2 | 14 GB | 140 GB |\n",
    "| Gradients (bf16) | 2 | 14 GB | 140 GB |\n",
    "| Adam momentum `m` (fp32) | 4 | 28 GB | 280 GB |\n",
    "| Adam variance `v` (fp32) | 4 | 28 GB | 280 GB |\n",
    "| Adam master weights (fp32) | 4 | 28 GB | 280 GB |\n",
    "| **Total per GPU (no sharding)** | **16** | **112 GB** | **1,120 GB** |\n",
    "\n",
    "Plus **activation memory** (depends on batch size and sequence length).\n",
    "\n",
    "### The Insight\n",
    "\n",
    "In standard DDP (DistributedDataParallel), **every GPU holds a complete copy** of all this state.\n",
    "With 8 GPUs, you have 8 identical copies of the optimizer states \u2014 that's **7 copies wasted**.\n",
    "\n",
    "ZeRO eliminates this redundancy by **partitioning** (sharding) state across GPUs.\n",
    "\n",
    "## The 4 Stages of ZeRO\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     What Each GPU Stores                         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502          \u2502 Parameters \u2502 Gradients  \u2502 Optimizer (m, v, master)   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 ZeRO-0   \u2502 FULL copy  \u2502 FULL copy  \u2502 FULL copy                  \u2502\n",
    "\u2502 (= DDP)  \u2502            \u2502            \u2502                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 ZeRO-1   \u2502 FULL copy  \u2502 FULL copy  \u2502 SHARDED (1/N)              \u2502\n",
    "\u2502          \u2502            \u2502            \u2502 Saves ~4\u00d7 on optimizer     \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 ZeRO-2   \u2502 FULL copy  \u2502 SHARDED    \u2502 SHARDED (1/N)              \u2502\n",
    "\u2502          \u2502            \u2502 (1/N)      \u2502 Saves ~8\u00d7 on opt+grad      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 ZeRO-3   \u2502 SHARDED    \u2502 SHARDED    \u2502 SHARDED (1/N)              \u2502\n",
    "\u2502          \u2502 (1/N)      \u2502 (1/N)      \u2502 Saves ~16\u00d7 on everything   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Memory per GPU (7B model, 2 GPUs, bf16)\n",
    "\n",
    "```\n",
    "ZeRO-0:  14 + 14 + 84 = 112 GB  (everything replicated)\n",
    "ZeRO-1:  14 + 14 + 42 =  70 GB  (optimizer sharded \u2192 84/2 = 42)\n",
    "ZeRO-2:  14 +  7 + 42 =  63 GB  (+ grads sharded \u2192 14/2 = 7)\n",
    "ZeRO-3:   7 +  7 + 42 =  56 GB  (+ params sharded \u2192 14/2 = 7)\n",
    "```\n",
    "\n",
    "## Communication Cost at Each Stage\n",
    "\n",
    "There's no free lunch \u2014 less memory means more communication:\n",
    "\n",
    "| Stage | Communication Pattern | Volume (per step) |\n",
    "|-------|----------------------|-------------------|\n",
    "| ZeRO-0 | All-reduce on gradients (during backward) | 2\u03a6 |\n",
    "| ZeRO-1 | All-reduce on gradients + all-gather optimizer results | 2\u03a6 |\n",
    "| ZeRO-2 | Reduce-scatter gradients + all-gather updated params | 2\u03a6 |\n",
    "| ZeRO-3 | All-gather params before fwd+bwd + reduce-scatter grads | 3\u03a6 |\n",
    "\n",
    "Where \u03a6 = total parameter bytes. ZeRO-3 communicates **50% more** than ZeRO-0/1/2.\n",
    "This is why ZeRO-3 is slower \u2014 the extra communication is the price of lower memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q deepspeed transformers datasets tokenizers accelerate matplotlib\n",
    "\n",
    "# Optional: Flash Attention 2 (strongly recommended for B200/H100)\n",
    "pip install -q flash-attn --no-build-isolation 2>/dev/null || echo 'Flash Attention not installed (non-critical)'\n",
    "\n",
    "echo ''\n",
    "echo '=== Environment Check ==='\n",
    "python -c '\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    mem = torch.cuda.get_device_properties(i).total_mem / 1e9\n",
    "    print(f\"  GPU {i}: {name} ({mem:.0f} GB)\")\n",
    "'\n",
    "python -c 'import deepspeed; print(f\"DeepSpeed: {deepspeed.__version__}\")'\n",
    "echo '=== Ready! ==='\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-download model and data (so timing measurements are clean)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL_NAME = 'EleutherAI/pythia-6.9b-deduped'\n",
    "# For smaller GPUs (A100 40GB), use:\n",
    "# MODEL_NAME = 'EleutherAI/pythia-2.8b-deduped'\n",
    "\n",
    "print(f'Downloading tokenizer for {MODEL_NAME}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f'Downloading model weights for {MODEL_NAME}...')\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype='auto')\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'  Parameters: {n_params:,} ({n_params/1e9:.2f}B)')\n",
    "del model  # free memory\n",
    "\n",
    "print(f'Downloading Alpaca dataset...')\n",
    "ds = load_dataset('tatsu-lab/alpaca')\n",
    "print(f'  Training examples: {len(ds[\"train\"]):,}')\n",
    "\n",
    "import torch; torch.cuda.empty_cache()\n",
    "print('\\n\u2713 Everything cached and ready!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Understanding the Training Script\n",
    "\n",
    "Before running experiments, let's walk through the key pieces of `deepspeed_train.py`.\n",
    "\n",
    "## 3.1 \u2014 DeepSpeed Initialization\n",
    "\n",
    "The core of DeepSpeed is `deepspeed.initialize()`. This single call:\n",
    "1. Reads your JSON config to determine the ZeRO stage\n",
    "2. Wraps your model for distributed training\n",
    "3. Creates the optimizer with the appropriate sharding\n",
    "4. Sets up gradient communication hooks\n",
    "\n",
    "```python\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    model=model,                    # Your vanilla PyTorch model\n",
    "    model_parameters=model.parameters(),\n",
    "    config=args.deepspeed_config,   # JSON file controls EVERYTHING\n",
    ")\n",
    "```\n",
    "\n",
    "The returned `model_engine` replaces your model. You call:\n",
    "- `model_engine(input_ids=...)` for forward pass\n",
    "- `model_engine.backward(loss)` instead of `loss.backward()`\n",
    "- `model_engine.step()` instead of `optimizer.step()`\n",
    "\n",
    "## 3.2 \u2014 The Config is King\n",
    "\n",
    "Here's what changes between ZeRO stages \u2014 it's ONLY the JSON config:\n",
    "\n",
    "**ZeRO-0** (baseline DDP):\n",
    "```json\n",
    "{ \"zero_optimization\": { \"stage\": 0 } }\n",
    "```\n",
    "\n",
    "**ZeRO-1** (shard optimizer):\n",
    "```json\n",
    "{ \"zero_optimization\": { \"stage\": 1 } }\n",
    "```\n",
    "\n",
    "**ZeRO-2** (shard optimizer + gradients):\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"overlap_comm\": true,        // Overlap comm with compute\n",
    "    \"reduce_scatter\": true,      // Use reduce-scatter (efficient)\n",
    "    \"contiguous_gradients\": true  // Coalesce grads in memory\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**ZeRO-3** (shard everything):\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"stage3_prefetch_bucket_size\": 2e8,          // Prefetch next layer's params\n",
    "    \"stage3_param_persistence_threshold\": 1e5,   // Keep tiny params local\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true  // Consolidate for saving\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**The training script is IDENTICAL across all stages.** This is DeepSpeed's design philosophy:\n",
    "separate the parallelism strategy from the training code.\n",
    "\n",
    "## 3.3 \u2014 Saving with ZeRO-3\n",
    "\n",
    "With ZeRO-3, parameters are distributed across GPUs \u2014 no single GPU has the full model.\n",
    "To save, DeepSpeed must **consolidate** parameters from all ranks:\n",
    "\n",
    "```python\n",
    "if zero_stage == 3:\n",
    "    model_engine.save_16bit_model(output_path)  # Gathers from all GPUs\n",
    "else:\n",
    "    model_engine.module.save_pretrained(output_path)  # Local save\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Running the Experiments\n",
    "\n",
    "We'll train Pythia-6.9B on Alpaca with each ZeRO stage and compare:\n",
    "- **Peak GPU memory** \u2014 how much does sharding save?\n",
    "- **Throughput** \u2014 what's the communication overhead?\n",
    "- **Loss convergence** \u2014 do all stages produce the same result?\n",
    "\n",
    "Each run does **500 steps** (enough to see convergence trends and measure throughput).\n",
    "At the end, we'll do a **full training run** with the best config for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: ZeRO Stage 0 (No Sharding = DDP)\n",
    "\n",
    "This is the **baseline**. Every GPU holds a complete copy of everything.\n",
    "All gradient synchronization uses all-reduce during backward (overlapped).\n",
    "\n",
    "**Expected:** Highest memory usage, fastest throughput (minimal communication).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess, time, os\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "MODEL = 'EleutherAI/pythia-6.9b-deduped'\n",
    "# MODEL = 'EleutherAI/pythia-2.8b-deduped'  # Use this for smaller GPUs\n",
    "STEPS = 500  # Comparison runs: 500 steps each\n",
    "\n",
    "print('\u2550'*72)\n",
    "print('  ZeRO Stage 0: NO Sharding (DDP Baseline)')\n",
    "print('\u2550'*72)\n",
    "\n",
    "t0 = time.time()\n",
    "result = subprocess.run(\n",
    "    f'deepspeed --num_gpus=2 deepspeed_train.py '\n",
    "    f'--deepspeed_config ds_zero0.json '\n",
    "    f'--model_name {MODEL} '\n",
    "    f'--max_steps {STEPS} --batch_size 4 --grad_accum 2 '\n",
    "    f'--output_dir ./output --run_name zero0',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(result.stdout[-3000:])  # Last 3000 chars\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n",
    "else:\n",
    "    print(f'\\n  \u2713 ZeRO-0 complete in {elapsed:.0f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: ZeRO Stage 1 (Shard Optimizer States)\n",
    "\n",
    "Now the Adam optimizer states (momentum `m`, variance `v`, and fp32 master weights)\n",
    "are **partitioned** across GPUs. Each GPU stores only 1/N of them.\n",
    "\n",
    "After `optimizer.step()`, each GPU updates its partition, then the results are\n",
    "all-gathered so every GPU has the updated parameters.\n",
    "\n",
    "**Expected:** Lower memory (optimizer is the biggest component!), similar throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('\u2550'*72)\n",
    "print('  ZeRO Stage 1: Shard Optimizer States')\n",
    "print('\u2550'*72)\n",
    "\n",
    "t0 = time.time()\n",
    "result = subprocess.run(\n",
    "    f'deepspeed --num_gpus=2 deepspeed_train.py '\n",
    "    f'--deepspeed_config ds_zero1.json '\n",
    "    f'--model_name {MODEL} '\n",
    "    f'--max_steps {STEPS} --batch_size 4 --grad_accum 2 '\n",
    "    f'--output_dir ./output --run_name zero1',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(result.stdout[-3000:])\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n",
    "else:\n",
    "    print(f'\\n  \u2713 ZeRO-1 complete in {elapsed:.0f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: ZeRO Stage 2 (Shard Optimizer + Gradients)\n",
    "\n",
    "In addition to optimizer sharding, **gradients are now reduce-scattered** instead of\n",
    "all-reduced. This means:\n",
    "1. During backward, each GPU computes full gradients (same as before)\n",
    "2. Instead of all-reduce (everyone gets all gradients), we use **reduce-scatter**:\n",
    "   each GPU receives only its 1/N partition of the averaged gradients\n",
    "3. Each GPU updates its 1/N partition of the optimizer\n",
    "4. Updated parameters are all-gathered\n",
    "\n",
    "**Key insight:** In ZeRO-2, `reduce_scatter + all_gather` replaces `all_reduce`.\n",
    "The total communication volume is the same (2\u03a6), but gradients don't persist in full.\n",
    "\n",
    "**This is the sweet spot for most use cases** \u2014 good memory savings, minimal throughput loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('\u2550'*72)\n",
    "print('  ZeRO Stage 2: Shard Optimizer + Gradients')\n",
    "print('\u2550'*72)\n",
    "\n",
    "t0 = time.time()\n",
    "result = subprocess.run(\n",
    "    f'deepspeed --num_gpus=2 deepspeed_train.py '\n",
    "    f'--deepspeed_config ds_zero2.json '\n",
    "    f'--model_name {MODEL} '\n",
    "    f'--max_steps {STEPS} --batch_size 4 --grad_accum 2 '\n",
    "    f'--output_dir ./output --run_name zero2',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(result.stdout[-3000:])\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n",
    "else:\n",
    "    print(f'\\n  \u2713 ZeRO-2 complete in {elapsed:.0f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: ZeRO Stage 3 (Shard Everything)\n",
    "\n",
    "The most aggressive stage: **parameters themselves are distributed**. No GPU holds\n",
    "the full model at any point during training.\n",
    "\n",
    "The lifecycle of a parameter in ZeRO-3:\n",
    "\n",
    "```\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502  Parameters   \u2502      \u2502  Parameters   \u2502      \u2502  Parameters   \u2502\n",
    "  \u2502  (sharded     \u2502\u2500\u2500\u2500\u2500\u2500>\u2502  (all-gather  \u2502\u2500\u2500\u2500\u2500\u2500>\u2502  (use in     \u2502\n",
    "  \u2502   1/N each)   \u2502      \u2502   full layer) \u2502      \u2502   forward)    \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                                      \u2502\n",
    "                                                      v\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502  Free full    \u2502      \u2502  Reduce-     \u2502      \u2502  Compute     \u2502\n",
    "  \u2502  params,      \u2502<\u2500\u2500\u2500\u2500\u2500\u2502  scatter     \u2502<\u2500\u2500\u2500\u2500\u2500\u2502  gradients   \u2502\n",
    "  \u2502  keep shard   \u2502      \u2502  gradients   \u2502      \u2502  (backward)  \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "This adds **1\u03a6 extra communication** (all-gather before forward) on top of ZeRO-2.\n",
    "\n",
    "**Use ZeRO-3 when the model is too large to hold full parameters on each GPU.**\n",
    "With 2 GPUs, a 7B model fits without ZeRO-3. But a 70B model REQUIRES it.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('\u2550'*72)\n",
    "print('  ZeRO Stage 3: Shard Everything')\n",
    "print('\u2550'*72)\n",
    "\n",
    "t0 = time.time()\n",
    "result = subprocess.run(\n",
    "    f'deepspeed --num_gpus=2 deepspeed_train.py '\n",
    "    f'--deepspeed_config ds_zero3.json '\n",
    "    f'--model_name {MODEL} '\n",
    "    f'--max_steps {STEPS} --batch_size 4 --grad_accum 2 '\n",
    "    f'--output_dir ./output --run_name zero3',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(result.stdout[-3000:])\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n",
    "else:\n",
    "    print(f'\\n  \u2713 ZeRO-3 complete in {elapsed:.0f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Results Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "result = subprocess.run('python compare_zero_stages.py ./output',\n",
    "                        shell=True, capture_output=True, text=True, cwd='/workspace')\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-2000:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "for f in sorted(glob.glob('/workspace/output/results_zero*.json')):\n",
    "    with open(f) as fh:\n",
    "        results.append(json.load(fh))\n",
    "\n",
    "if not results:\n",
    "    print('No results found \u2014 run the experiments first!')\n",
    "else:\n",
    "    results.sort(key=lambda r: r['zero_stage'])\n",
    "    stages = [f\"ZeRO-{r['zero_stage']}\" for r in results]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Memory\n",
    "    mems = [r['peak_memory_gb'] for r in results]\n",
    "    colors = ['#ef4444','#f59e0b','#22c55e','#4a9eed']\n",
    "    axes[0].bar(stages, mems, color=colors[:len(stages)], edgecolor='white', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Peak Memory (GB)')\n",
    "    axes[0].set_title('GPU Memory', fontweight='bold')\n",
    "    for i, v in enumerate(mems):\n",
    "        axes[0].text(i, v+0.5, f'{v:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "    # Throughput\n",
    "    tputs = [r['avg_throughput_tok_s'] for r in results]\n",
    "    axes[1].bar(stages, tputs, color=colors[:len(stages)], edgecolor='white', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Tokens/sec')\n",
    "    axes[1].set_title('Throughput', fontweight='bold')\n",
    "    for i, v in enumerate(tputs):\n",
    "        axes[1].text(i, v+100, f'{v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "    # Loss curves\n",
    "    for r in results:\n",
    "        hist = r.get('loss_history', [])\n",
    "        if hist:\n",
    "            axes[2].plot(range(1, len(hist)+1), hist, label=f\"ZeRO-{r['zero_stage']}\", linewidth=2)\n",
    "    axes[2].set_xlabel('Step')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('Training Loss (should overlap!)', fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle('DeepSpeed ZeRO Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "**Memory:** You should see a clear staircase \u2014 ZeRO-0 highest, ZeRO-3 lowest.\n",
    "For a 7B model on 2 GPUs, expect roughly:\n",
    "- ZeRO-0: ~80-110 GB (full replication)\n",
    "- ZeRO-1: ~60-70 GB (optimizer sharded)\n",
    "- ZeRO-2: ~50-60 GB (+ gradient sharded)\n",
    "- ZeRO-3: ~30-50 GB (+ params sharded)\n",
    "\n",
    "**Throughput:** ZeRO-0/1/2 should be within ~5% of each other. ZeRO-3 will be\n",
    "noticeably slower (20-40%) due to the extra parameter all-gather communication.\n",
    "\n",
    "**Loss curves:** These should be IDENTICAL (or very close). The math is the same \u2014\n",
    "ZeRO only changes how memory is managed, not what's computed. Small differences\n",
    "are due to floating-point ordering in reductions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Full Training Run (for Inference Quality)\n",
    "\n",
    "The 500-step comparison runs are too short for meaningful instruction-following.\n",
    "Now let's do a proper training run with ZeRO-2 (best memory/speed trade-off).\n",
    "\n",
    "We'll train for **2000 steps** (~half an epoch of Alpaca), which takes about 15-30 minutes\n",
    "on 2\u00d7 B200.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('\u2550'*72)\n",
    "print('  FULL TRAINING: ZeRO-2, 2000 steps')\n",
    "print('  This will take 15-30 minutes...')\n",
    "print('\u2550'*72)\n",
    "\n",
    "import subprocess, time\n",
    "t0 = time.time()\n",
    "\n",
    "result = subprocess.run(\n",
    "    f'deepspeed --num_gpus=2 deepspeed_train.py '\n",
    "    f'--deepspeed_config ds_zero2.json '\n",
    "    f'--model_name {MODEL} '\n",
    "    f'--max_steps 2000 --batch_size 4 --grad_accum 2 '\n",
    "    f'--warmup_steps 100 --eval_interval 200 '\n",
    "    f'--output_dir ./output --run_name zero2_full '\n",
    "    f'--save_model',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(result.stdout[-5000:])\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n",
    "else:\n",
    "    print(f'\\n  \u2713 Full training complete in {elapsed:.0f}s ({elapsed/60:.1f} min)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Inference \u2014 Before vs After\n",
    "\n",
    "Let's see if the fine-tuning actually worked! We'll compare the base model's\n",
    "responses to the fine-tuned model's responses on the same prompts.\n",
    "\n",
    "The base model (Pythia-6.9B) was pretrained on web text \u2014 it can generate coherent\n",
    "English but doesn't follow instructions well. After Alpaca fine-tuning, it should\n",
    "produce structured, helpful responses to instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "\n",
    "print('\u2550'*72)\n",
    "print('  INFERENCE: Base vs Fine-tuned Comparison')\n",
    "print('\u2550'*72)\n",
    "\n",
    "result = subprocess.run(\n",
    "    f'python run_inference.py '\n",
    "    f'--model_path ./output/zero2_full '\n",
    "    f'--base_model {MODEL}',\n",
    "    shell=True, capture_output=True, text=True, cwd='/workspace'\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print('STDERR:', result.stderr[-3000:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: When to Use What \u2014 Decision Framework\n",
    "\n",
    "```\n",
    "                   Does model + optimizer fit on 1 GPU?\n",
    "                              \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    YES                  NO\n",
    "                    \u2502                    \u2502\n",
    "              Use ZeRO-0            Does model fit on 1 GPU\n",
    "              (plain DDP)           but optimizer doesn't?\n",
    "                                         \u2502\n",
    "                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                               YES                  NO\n",
    "                               \u2502                    \u2502\n",
    "                          Use ZeRO-1/2         Use ZeRO-3\n",
    "                          (shard optimizer)    (shard everything)\n",
    "                               \u2502\n",
    "                         Need max memory?\n",
    "                               \u2502\n",
    "                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                     YES                  NO\n",
    "                     \u2502                    \u2502\n",
    "                Use ZeRO-2          Use ZeRO-1\n",
    "                (most popular)      (simplest)\n",
    "```\n",
    "\n",
    "### Rules of Thumb\n",
    "\n",
    "| Scenario | Recommendation | Reason |\n",
    "|----------|---------------|--------|\n",
    "| 7B model, 4\u00d7 A100 80GB | ZeRO-1 or ZeRO-2 | Model fits, save optimizer memory for larger batches |\n",
    "| 13B model, 2\u00d7 A100 80GB | ZeRO-2 | Tight fit \u2014 need optimizer + gradient sharding |\n",
    "| 70B model, 8\u00d7 A100 80GB | ZeRO-3 | Model doesn't fit on one GPU |\n",
    "| 7B model, 2\u00d7 A100 40GB | ZeRO-2 or ZeRO-3 | Limited memory \u2014 need sharding |\n",
    "| Any model, want max throughput | ZeRO-0 or ZeRO-1 | Less communication overhead |\n",
    "\n",
    "### ZeRO-2 is the Default Choice for Most Teams\n",
    "\n",
    "It gives you significant memory savings with almost no throughput penalty.\n",
    "Only move to ZeRO-3 when you need the extra memory, and only stay at ZeRO-0\n",
    "when your model is small enough that memory isn't a concern.\n",
    "\n",
    "### Beyond ZeRO: When Data Parallelism Isn't Enough\n",
    "\n",
    "If you have 100+ GPUs, data parallelism alone hits a batch size wall.\n",
    "You need **3D parallelism**:\n",
    "- **Tensor Parallelism**: Split matrix multiplies across GPUs (within a node)\n",
    "- **Pipeline Parallelism**: Split layers across GPUs (across nodes)\n",
    "- **Data Parallelism (ZeRO)**: Replicate the above across groups of GPUs\n",
    "\n",
    "That's the next lecture.\n"
   ]
  }
 ]
}