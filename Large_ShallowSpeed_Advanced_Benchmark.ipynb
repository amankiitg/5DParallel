{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowSpeed Advanced Benchmark: 8 GPUs, Mixed Precision, Gradient Accumulation & Profiling\n",
    "\n",
    "An extended version of the ShallowSpeed GPU benchmark with:\n",
    "\n",
    "| Feature | What it adds |\n",
    "|---------|-------------|\n",
    "| **8 GPU scaling** | Benchmarks across 1, 2, 4, and 8 GPUs to see where communication overhead dominates |\n",
    "| **Mixed Precision (AMP)** | FP16/BF16 training — halves gradient size, so AllReduce transfers half the data |\n",
    "| **Gradient Accumulation** | Simulate larger effective batch sizes without more memory — fewer AllReduce calls per epoch |\n",
    "| **torch.profiler** | Actual GPU timeline traces showing compute vs NCCL overlap |\n",
    "| **Fixed loss logging** | Properly averages loss across all ranks (fixes the local-loss artifact from the original notebook) |\n",
    "\n",
    "## Hardware Requirements\n",
    "- **RunPod**: 2, 4, or 8 GPUs (e.g., 8x A100, 8x H100)\n",
    "- Works on any multi-GPU machine with NCCL support\n",
    "- For NVLink vs PCIe comparison: run this notebook on both SXM and PCIe instances, then compare the saved JSON results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "print(f\"NCCL available  : {torch.distributed.is_nccl_available()}\")\n",
    "print(f\"BF16 supported  : {torch.cuda.is_bf16_supported() if torch.cuda.is_available() else 'N/A'}\")\n",
    "print(f\"GPUs found      : {NUM_GPUS}\")\n",
    "\n",
    "for i in range(NUM_GPUS):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    mem = getattr(props, 'total_memory', getattr(props, 'total_mem', 0)) / 1024**3\n",
    "    print(f\"  GPU {i}: {props.name} ({mem:.1f} GB)\")\n",
    "\n",
    "# Detect interconnect type\n",
    "if NUM_GPUS >= 2:\n",
    "    try:\n",
    "        result = !nvidia-smi topo -m 2>/dev/null | head -20\n",
    "        for line in result:\n",
    "            print(line)\n",
    "        if any('NV' in str(line) for line in result):\n",
    "            print(\"\\n>>> NVLink detected — expect fast AllReduce\")\n",
    "        else:\n",
    "            print(\"\\n>>> PCIe interconnect — AllReduce will be slower\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if NUM_GPUS < 2:\n",
    "    print(\"\\n⚠️  Only 1 GPU detected. Data-parallel benchmarks need >= 2 GPUs.\")\n",
    "    print(\"    Single GPU baseline + profiler will still run.\")\n",
    "\n",
    "# Store GPU name for results tagging\n",
    "GPU_NAME = torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'unknown'\n",
    "print(f\"\\nGPU tag for results: {GPU_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training Scripts\n",
    "\n",
    "All scripts now support:\n",
    "- `use_amp`: Enable mixed precision (FP16/BF16)\n",
    "- `grad_accum_steps`: Accumulate gradients over N micro-steps before AllReduce + optimizer step\n",
    "- **Fixed loss logging**: `dist.all_reduce` on the loss scalar so all ranks report the true global average\n",
    "\n",
    "### 2a. Shared Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile model_common.py\n\"\"\"\nShared model definition and dataset — used by all training scripts.\n\nModel sizes are designed to stress H100/A100 GPUs:\n  - base:   ~64M params  — minimum viable for multi-GPU benefit\n  - large:  ~250M params — clear scaling benefits\n  - xlarge: ~730M params — approaches billion-scale, strong scaling\n\nDeeper + wider = more compute per step = more room for parallelism to help.\n\"\"\"\nimport torch\nimport torch.nn as nn\n\n\ndef build_model(size='large'):\n    configs = {\n        'base':   [784, 4096, 4096, 4096, 4096, 2048, 1024, 10],              # ~64M params\n        'large':  [784, 8192, 8192, 8192, 8192, 4096, 2048, 10],              # ~250M params\n        'xlarge': [784, 16384, 16384, 16384, 8192, 4096, 2048, 10],           # ~730M params\n    }\n    sizes = configs[size]\n    layers = []\n    for i in range(len(sizes) - 1):\n        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n        if i < len(sizes) - 2:\n            layers.append(nn.ReLU())\n    model = nn.Sequential(*layers)\n    n_params = sum(p.numel() for p in model.parameters())\n    return model, n_params\n\n\ndef make_dataset(n_samples=65536, n_features=784, n_classes=10):\n    \"\"\"Larger dataset to increase compute per epoch.\"\"\"\n    torch.manual_seed(42)\n    X = torch.randn(n_samples, n_features)\n    y = torch.randint(0, n_classes, (n_samples,))\n    return X, y\n\n\nprint(\"model_common.py loaded OK\")\nfor sz in ['base', 'large', 'xlarge']:\n    _, n = build_model(sz)\n    print(f\"  {sz:8s}: {n:>12,} params\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Single GPU Baseline\n",
    "\n",
    "Now supports mixed precision and gradient accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_single_gpu.py\n",
    "\"\"\"\n",
    "SINGLE GPU BASELINE — with AMP + gradient accumulation support.\n",
    "\"\"\"\n",
    "import torch, torch.nn as nn, time, json, sys, os\n",
    "from model_common import build_model, make_dataset\n",
    "\n",
    "def main():\n",
    "    config = json.loads(sys.argv[1])\n",
    "    device = torch.device('cuda:0')\n",
    "    use_amp = config.get('use_amp', False)\n",
    "    grad_accum_steps = config.get('grad_accum_steps', 1)\n",
    "    amp_dtype = torch.bfloat16 if (use_amp and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    model, n_params = build_model(config['model_size'])\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and amp_dtype == torch.float16))\n",
    "\n",
    "    X, y = make_dataset(config['n_samples'])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    bs = config['batch_size']\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        idx = torch.randint(0, len(X), (bs,))\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "            out = model(X[idx])\n",
    "            loss_fn(out, y[idx]).backward()\n",
    "        optimizer.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    epoch_times, losses = [], []\n",
    "    for epoch in range(config['n_epochs']):\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        epoch_loss, nb = 0.0, 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Build list of batches\n",
    "        batch_starts = list(range(0, len(X), bs))\n",
    "\n",
    "        for step_idx, start in enumerate(batch_starts):\n",
    "            xb = X[start:start+bs]\n",
    "            yb = y[start:start+bs]\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out, yb) / grad_accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_loss += loss.item() * grad_accum_steps\n",
    "            nb += 1\n",
    "\n",
    "            # Step every grad_accum_steps or at end of epoch\n",
    "            if (step_idx + 1) % grad_accum_steps == 0 or (step_idx + 1) == len(batch_starts):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_times.append(time.perf_counter() - t0)\n",
    "        losses.append(epoch_loss / nb)\n",
    "\n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for start in range(0, len(X), bs):\n",
    "            out = model(X[start:start+bs])\n",
    "            correct += (out.argmax(1) == y[start:start+bs]).sum().item()\n",
    "    acc = correct / len(X)\n",
    "\n",
    "    print('RESULTS_JSON:' + json.dumps({\n",
    "        'mode': 'single_gpu', 'n_gpus': 1, 'n_params': n_params,\n",
    "        'model_size': config['model_size'],\n",
    "        'use_amp': use_amp, 'grad_accum_steps': grad_accum_steps,\n",
    "        'epoch_times': epoch_times, 'losses': losses,\n",
    "        'final_accuracy': acc,\n",
    "        'avg_epoch_time': sum(epoch_times) / len(epoch_times),\n",
    "        'total_time': sum(epoch_times),\n",
    "        'comm_time': 0.0,\n",
    "    }))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Naive Data-Parallel (Non-Interleaved AllReduce)\n",
    "\n",
    "Full forward + full backward, THEN blocking AllReduce on all gradients.\n",
    "Now with AMP, gradient accumulation, and **fixed global loss logging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_dp_naive.py\n",
    "\"\"\"\n",
    "DATA-PARALLEL: NAIVE (NON-INTERLEAVED)\n",
    "With AMP + gradient accumulation + fixed global loss averaging.\n",
    "\"\"\"\n",
    "import torch, torch.nn as nn, torch.distributed as dist\n",
    "import time, json, sys, os\n",
    "from model_common import build_model, make_dataset\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f'cuda:{rank}')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    config = json.loads(sys.argv[1])\n",
    "    use_amp = config.get('use_amp', False)\n",
    "    grad_accum_steps = config.get('grad_accum_steps', 1)\n",
    "    amp_dtype = torch.bfloat16 if (use_amp and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model, n_params = build_model(config['model_size'])\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and amp_dtype == torch.float16))\n",
    "\n",
    "    X_all, y_all = make_dataset(config['n_samples'])\n",
    "    X_all, y_all = X_all.to(device), y_all.to(device)\n",
    "    bs = config['batch_size']\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        idx = torch.randint(0, len(X_all), (bs // world_size,))\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "            out = model(X_all[idx])\n",
    "            loss_fn(out, y_all[idx]).backward()\n",
    "        optimizer.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "    dist.barrier()\n",
    "\n",
    "    epoch_times, losses, comm_times = [], [], []\n",
    "    for epoch in range(config['n_epochs']):\n",
    "        torch.cuda.synchronize()\n",
    "        dist.barrier()\n",
    "        t0 = time.perf_counter()\n",
    "        epoch_loss, nb, epoch_comm = 0.0, 0, 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_starts = list(range(0, len(X_all), bs))\n",
    "\n",
    "        for step_idx, start in enumerate(batch_starts):\n",
    "            xb = X_all[start:start+bs]\n",
    "            yb = y_all[start:start+bs]\n",
    "\n",
    "            # Shard the batch across GPUs\n",
    "            chunk = len(xb) // world_size\n",
    "            s = rank * chunk\n",
    "            e = s + chunk if rank < world_size - 1 else len(xb)\n",
    "            x_local, y_local = xb[s:e], yb[s:e]\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(x_local)\n",
    "                loss = loss_fn(out, y_local) / grad_accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # --- Fixed global loss logging ---\n",
    "            loss_val = loss.detach() * grad_accum_steps\n",
    "            dist.all_reduce(loss_val, op=dist.ReduceOp.SUM)\n",
    "            epoch_loss += (loss_val.item() / world_size)\n",
    "            nb += 1\n",
    "\n",
    "            # AllReduce + step every grad_accum_steps or at end\n",
    "            if (step_idx + 1) % grad_accum_steps == 0 or (step_idx + 1) == len(batch_starts):\n",
    "                # Unscale before manual allreduce\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # === COMMUNICATION: AllReduce ALL gradients (BLOCKING) ===\n",
    "                torch.cuda.synchronize()\n",
    "                tc0 = time.perf_counter()\n",
    "\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n",
    "                        param.grad /= world_size\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_comm += time.perf_counter() - tc0\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_times.append(time.perf_counter() - t0)\n",
    "        losses.append(epoch_loss / max(nb, 1))\n",
    "        comm_times.append(epoch_comm)\n",
    "\n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for start in range(0, len(X_all), bs):\n",
    "            out = model(X_all[start:start+bs])\n",
    "            correct += (out.argmax(1) == y_all[start:start+bs]).sum().item()\n",
    "    acc = correct / len(X_all)\n",
    "\n",
    "    if rank == 0:\n",
    "        print('RESULTS_JSON:' + json.dumps({\n",
    "            'mode': 'dp_naive', 'n_gpus': world_size, 'n_params': n_params,\n",
    "            'model_size': config['model_size'],\n",
    "            'use_amp': use_amp, 'grad_accum_steps': grad_accum_steps,\n",
    "            'epoch_times': epoch_times, 'losses': losses,\n",
    "            'comm_times': comm_times,\n",
    "            'final_accuracy': acc,\n",
    "            'avg_epoch_time': sum(epoch_times) / len(epoch_times),\n",
    "            'avg_comm_time': sum(comm_times) / len(comm_times),\n",
    "            'total_time': sum(epoch_times),\n",
    "            'comm_time': sum(comm_times),\n",
    "        }))\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Interleaved Data-Parallel (Non-blocking AllReduce During Backward)\n",
    "\n",
    "Per-layer async AllReduce during backward pass. With AMP + gradient accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_dp_interleaved.py\n",
    "\"\"\"\n",
    "DATA-PARALLEL: INTERLEAVED (Non-blocking AllReduce during backward)\n",
    "With AMP + gradient accumulation + fixed global loss averaging.\n",
    "\n",
    "Gradient hooks fire async AllReduce per-layer ONLY on the accumulation\n",
    "step (when we actually need to sync). During accumulation micro-steps,\n",
    "hooks are disabled so gradients accumulate locally.\n",
    "\"\"\"\n",
    "import torch, torch.nn as nn, torch.distributed as dist\n",
    "import time, json, sys, os\n",
    "from model_common import build_model, make_dataset\n",
    "\n",
    "\n",
    "class InterleavedDP:\n",
    "    def __init__(self, model, world_size):\n",
    "        self.model = model\n",
    "        self.world_size = world_size\n",
    "        self._handles = []\n",
    "        self._sync_enabled = True  # Toggle for grad accumulation\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.register_post_accumulate_grad_hook(self._make_hook(param))\n",
    "\n",
    "    def _make_hook(self, param):\n",
    "        def hook(p):\n",
    "            if self._sync_enabled:\n",
    "                handle = dist.all_reduce(p.grad, op=dist.ReduceOp.SUM, async_op=True)\n",
    "                self._handles.append((handle, p))\n",
    "        return hook\n",
    "\n",
    "    def finish_allreduce(self):\n",
    "        for handle, param in self._handles:\n",
    "            handle.wait()\n",
    "            param.grad /= self.world_size\n",
    "        self._handles.clear()\n",
    "\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f'cuda:{rank}')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    config = json.loads(sys.argv[1])\n",
    "    use_amp = config.get('use_amp', False)\n",
    "    grad_accum_steps = config.get('grad_accum_steps', 1)\n",
    "    amp_dtype = torch.bfloat16 if (use_amp and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model, n_params = build_model(config['model_size'])\n",
    "    model = model.to(device)\n",
    "    dp = InterleavedDP(model, world_size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and amp_dtype == torch.float16))\n",
    "\n",
    "    X_all, y_all = make_dataset(config['n_samples'])\n",
    "    X_all, y_all = X_all.to(device), y_all.to(device)\n",
    "    bs = config['batch_size']\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        idx = torch.randint(0, len(X_all), (bs // world_size,))\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "            out = model(X_all[idx])\n",
    "            loss_fn(out, y_all[idx]).backward()\n",
    "        dp.finish_allreduce()\n",
    "        optimizer.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "    dist.barrier()\n",
    "\n",
    "    epoch_times, losses = [], []\n",
    "    for epoch in range(config['n_epochs']):\n",
    "        torch.cuda.synchronize()\n",
    "        dist.barrier()\n",
    "        t0 = time.perf_counter()\n",
    "        epoch_loss, nb = 0.0, 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_starts = list(range(0, len(X_all), bs))\n",
    "\n",
    "        for step_idx, start in enumerate(batch_starts):\n",
    "            xb = X_all[start:start+bs]\n",
    "            yb = y_all[start:start+bs]\n",
    "\n",
    "            chunk = len(xb) // world_size\n",
    "            s = rank * chunk\n",
    "            e = s + chunk if rank < world_size - 1 else len(xb)\n",
    "            x_local, y_local = xb[s:e], yb[s:e]\n",
    "\n",
    "            is_sync_step = ((step_idx + 1) % grad_accum_steps == 0) or ((step_idx + 1) == len(batch_starts))\n",
    "\n",
    "            # Only fire AllReduce hooks on the sync step\n",
    "            dp._sync_enabled = is_sync_step\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(x_local)\n",
    "                loss = loss_fn(out, y_local) / grad_accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # --- Fixed global loss logging ---\n",
    "            loss_val = loss.detach() * grad_accum_steps\n",
    "            dist.all_reduce(loss_val, op=dist.ReduceOp.SUM)\n",
    "            epoch_loss += (loss_val.item() / world_size)\n",
    "            nb += 1\n",
    "\n",
    "            if is_sync_step:\n",
    "                scaler.unscale_(optimizer)\n",
    "                dp.finish_allreduce()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_times.append(time.perf_counter() - t0)\n",
    "        losses.append(epoch_loss / max(nb, 1))\n",
    "\n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for start in range(0, len(X_all), bs):\n",
    "            out = model(X_all[start:start+bs])\n",
    "            correct += (out.argmax(1) == y_all[start:start+bs]).sum().item()\n",
    "    acc = correct / len(X_all)\n",
    "\n",
    "    if rank == 0:\n",
    "        print('RESULTS_JSON:' + json.dumps({\n",
    "            'mode': 'dp_interleaved', 'n_gpus': world_size, 'n_params': n_params,\n",
    "            'model_size': config['model_size'],\n",
    "            'use_amp': use_amp, 'grad_accum_steps': grad_accum_steps,\n",
    "            'epoch_times': epoch_times, 'losses': losses,\n",
    "            'final_accuracy': acc,\n",
    "            'avg_epoch_time': sum(epoch_times) / len(epoch_times),\n",
    "            'total_time': sum(epoch_times),\n",
    "        }))\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. PyTorch DDP (Reference)\n",
    "\n",
    "Production-grade interleaved AllReduce with gradient bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_ddp_builtin.py\n",
    "\"\"\"\n",
    "PyTorch DDP — with AMP + gradient accumulation + fixed loss.\n",
    "Uses no_sync() context manager to skip AllReduce during accumulation steps.\n",
    "\"\"\"\n",
    "import torch, torch.nn as nn, torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from contextlib import nullcontext\n",
    "import time, json, sys, os\n",
    "from model_common import build_model, make_dataset\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f'cuda:{rank}')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    config = json.loads(sys.argv[1])\n",
    "    use_amp = config.get('use_amp', False)\n",
    "    grad_accum_steps = config.get('grad_accum_steps', 1)\n",
    "    amp_dtype = torch.bfloat16 if (use_amp and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model, n_params = build_model(config['model_size'])\n",
    "    model = model.to(device)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and amp_dtype == torch.float16))\n",
    "\n",
    "    X_all, y_all = make_dataset(config['n_samples'])\n",
    "    X_all, y_all = X_all.to(device), y_all.to(device)\n",
    "    bs = config['batch_size']\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        idx = torch.randint(0, len(X_all), (bs // world_size,))\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "            out = model(X_all[idx])\n",
    "            loss_fn(out, y_all[idx]).backward()\n",
    "        optimizer.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "    dist.barrier()\n",
    "\n",
    "    epoch_times, losses = [], []\n",
    "    for epoch in range(config['n_epochs']):\n",
    "        torch.cuda.synchronize()\n",
    "        dist.barrier()\n",
    "        t0 = time.perf_counter()\n",
    "        epoch_loss, nb = 0.0, 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_starts = list(range(0, len(X_all), bs))\n",
    "\n",
    "        for step_idx, start in enumerate(batch_starts):\n",
    "            xb = X_all[start:start+bs]\n",
    "            yb = y_all[start:start+bs]\n",
    "\n",
    "            chunk = len(xb) // world_size\n",
    "            s = rank * chunk\n",
    "            e = s + chunk if rank < world_size - 1 else len(xb)\n",
    "            x_local, y_local = xb[s:e], yb[s:e]\n",
    "\n",
    "            is_sync_step = ((step_idx + 1) % grad_accum_steps == 0) or ((step_idx + 1) == len(batch_starts))\n",
    "\n",
    "            # DDP's no_sync() skips AllReduce during accumulation steps\n",
    "            sync_context = nullcontext() if is_sync_step else model.no_sync()\n",
    "\n",
    "            with sync_context:\n",
    "                with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "                    out = model(x_local)\n",
    "                    loss = loss_fn(out, y_local) / grad_accum_steps\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "            # --- Fixed global loss logging ---\n",
    "            loss_val = loss.detach() * grad_accum_steps\n",
    "            dist.all_reduce(loss_val, op=dist.ReduceOp.SUM)\n",
    "            epoch_loss += (loss_val.item() / world_size)\n",
    "            nb += 1\n",
    "\n",
    "            if is_sync_step:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_times.append(time.perf_counter() - t0)\n",
    "        losses.append(epoch_loss / max(nb, 1))\n",
    "\n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for start in range(0, len(X_all), bs):\n",
    "            out = model(X_all[start:start+bs])\n",
    "            correct += (out.argmax(1) == y_all[start:start+bs]).sum().item()\n",
    "    acc = correct / len(X_all)\n",
    "\n",
    "    if rank == 0:\n",
    "        print('RESULTS_JSON:' + json.dumps({\n",
    "            'mode': 'ddp_builtin', 'n_gpus': world_size, 'n_params': n_params,\n",
    "            'model_size': config['model_size'],\n",
    "            'use_amp': use_amp, 'grad_accum_steps': grad_accum_steps,\n",
    "            'epoch_times': epoch_times, 'losses': losses,\n",
    "            'final_accuracy': acc,\n",
    "            'avg_epoch_time': sum(epoch_times) / len(epoch_times),\n",
    "            'total_time': sum(epoch_times),\n",
    "        }))\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Benchmark Runner\n\nTrimmed matrix (~50 runs) that still covers the full range:\n- **3 model sizes**: base (64M), large (250M), xlarge (730M) — sized to stress H100s\n- **GPU counts**: 1, 2, 4 (up to 8 if available)\n- **Precision**: FP32 and AMP for all configs\n- **Gradient accumulation**: GA=1 for all, GA=4 added for `large` model only (to show the effect without doubling runtime)\n- **65K samples, batch size 4096** — enough data to keep GPUs busy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile run_benchmarks.py\nimport subprocess, json, sys, os, torch\n\nNUM_GPUS = torch.cuda.device_count()\nGPU_NAME = torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'unknown'\nprint(f\"Detected {NUM_GPUS} GPUs ({GPU_NAME})\")\n\ndef run(cmd):\n    print(f\"  CMD: {' '.join(cmd[:6])}...\")\n    r = subprocess.run(cmd, capture_output=True, text=True, timeout=900)\n    if r.returncode != 0:\n        print(f\"  ERROR (rc={r.returncode}): {r.stderr[:400]}\")\n        return None\n    for line in r.stdout.split('\\n'):\n        if line.startswith('RESULTS_JSON:'):\n            result = json.loads(line[len('RESULTS_JSON:'):])\n            result['gpu_name'] = GPU_NAME\n            return result\n    print(f\"  No RESULTS_JSON. stdout: {r.stdout[:200]}\")\n    return None\n\n\nbase = {\n    'n_samples': 65536,     # 2x larger dataset\n    'batch_size': 4096,     # bigger batches to saturate H100 compute\n    'lr': 0.01,\n    'n_epochs': 5,\n}\n\n# --- What to benchmark ---\n# Models sized for H100: base (64M), large (250M), xlarge (730M)\nmodel_sizes = ['base', 'large', 'xlarge']\ngpu_counts = [g for g in [2, 4, 8] if g <= NUM_GPUS]\namp_modes = [False, True]\n\n# Build the test matrix: ~50 runs\n# All models: FP32 + AMP, GA=1\n# Large model only: also GA=4 (to show gradient accumulation effect)\ntest_configs = []\nfor msz in model_sizes:\n    for use_amp in amp_modes:\n        test_configs.append((msz, use_amp, 1))\n        # Add GA=4 only for 'large' model\n        if msz == 'large':\n            test_configs.append((msz, use_amp, 4))\n\nall_results = []\nport_counter = 29500\ntotal_runs = len(test_configs) * (1 + len(gpu_counts) * 3)\nrun_num = 0\n\nprint(f\"\\nBenchmark plan: {len(test_configs)} configs × (1 single + {len(gpu_counts)} gpu_counts × 3 modes)\")\nprint(f\"Total estimated runs: ~{total_runs}\\n\")\n\nfor msz, use_amp, ga in test_configs:\n    cfg = {**base, 'model_size': msz, 'use_amp': use_amp, 'grad_accum_steps': ga}\n    cfg_str = json.dumps(cfg)\n    amp_tag = 'AMP' if use_amp else 'FP32'\n    ga_tag = f'GA={ga}'\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Model: {msz} | {amp_tag} | {ga_tag}\")\n    print(f\"{'='*60}\")\n\n    # --- Single GPU baseline ---\n    run_num += 1\n    print(f\"\\n  [{run_num}/{total_runs}] Single GPU baseline...\")\n    r = run(['python', 'train_single_gpu.py', cfg_str])\n    if r:\n        all_results.append(r)\n        print(f\"    avg_epoch={r['avg_epoch_time']:.4f}s\")\n\n    for ng in gpu_counts:\n        for mode_idx, (script, mode_name) in enumerate([\n            ('train_dp_naive.py', 'DP Naive'),\n            ('train_dp_interleaved.py', 'DP Interleaved'),\n            ('train_ddp_builtin.py', 'PyTorch DDP'),\n        ]):\n            port_counter += 1\n            run_num += 1\n            tr = ['torchrun', '--nproc_per_node', str(ng),\n                  '--master_port', str(port_counter)]\n\n            print(f\"\\n  [{run_num}/{total_runs}] {mode_name}, {ng} GPUs...\")\n            r = run(tr + [script, cfg_str])\n            if r:\n                all_results.append(r)\n                comm_str = f\", comm={r.get('avg_comm_time',0):.4f}s\" if 'avg_comm_time' in r else \"\"\n                print(f\"    avg_epoch={r['avg_epoch_time']:.4f}s{comm_str}\")\n\n# Save with GPU tag in filename\ngpu_tag = GPU_NAME.replace(' ', '_').replace('/', '_')\nfilename = f'benchmark_results_{gpu_tag}.json'\nwith open(filename, 'w') as f:\n    json.dump(all_results, f, indent=2)\nwith open('benchmark_results.json', 'w') as f:\n    json.dump(all_results, f, indent=2)\n\nprint(f\"\\n\\nDone! {len(all_results)} results saved to {filename}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_benchmarks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Profiler: GPU Timeline Traces\n",
    "\n",
    "This cell runs a few training steps with `torch.profiler` and exports Chrome traces.\n",
    "You can view them at `chrome://tracing` to see exactly when compute and NCCL kernels overlap.\n",
    "\n",
    "We profile 3 configs:\n",
    "1. **Naive DP** — you should see NCCL kernels AFTER compute kernels (no overlap)\n",
    "2. **Interleaved DP** — NCCL and compute kernels should overlap\n",
    "3. **PyTorch DDP** — similar overlap to interleaved, but with bucketed NCCL calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_profiler.py\n",
    "\"\"\"\n",
    "Profile one training step for each DP mode.\n",
    "Exports Chrome traces to ./profiles/\n",
    "\"\"\"\n",
    "import torch, torch.nn as nn, torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.profiler import profile, ProfilerActivity, schedule, tensorboard_trace_handler\n",
    "import json, sys, os\n",
    "from model_common import build_model, make_dataset\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f'cuda:{rank}')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    config = json.loads(sys.argv[1])\n",
    "    mode = config['mode']\n",
    "    use_amp = config.get('use_amp', False)\n",
    "    amp_dtype = torch.bfloat16 if (use_amp and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    os.makedirs('profiles', exist_ok=True)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model, _ = build_model(config.get('model_size', 'large'))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Setup based on mode\n",
    "    handles_list = []\n",
    "    if mode == 'ddp_builtin':\n",
    "        model = DDP(model, device_ids=[rank])\n",
    "    elif mode == 'dp_interleaved':\n",
    "        # Register interleaved hooks\n",
    "        for param in model.parameters():\n",
    "            def make_hook(p):\n",
    "                def hook(p_inner):\n",
    "                    h = dist.all_reduce(p_inner.grad, op=dist.ReduceOp.SUM, async_op=True)\n",
    "                    handles_list.append((h, p_inner))\n",
    "                return hook\n",
    "            param.register_post_accumulate_grad_hook(make_hook(param))\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    X_all, y_all = make_dataset(8192)\n",
    "    X_all, y_all = X_all.to(device), y_all.to(device)\n",
    "    bs = 1024\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        chunk = bs // world_size\n",
    "        idx = torch.randint(0, len(X_all), (chunk,))\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "            out = model(X_all[idx])\n",
    "            loss_fn(out, y_all[idx]).backward()\n",
    "        if mode == 'dp_interleaved':\n",
    "            for h, p in handles_list:\n",
    "                h.wait()\n",
    "                p.grad /= world_size\n",
    "            handles_list.clear()\n",
    "        optimizer.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "    dist.barrier()\n",
    "\n",
    "    amp_tag = 'amp' if use_amp else 'fp32'\n",
    "    trace_name = f'profiles/trace_{mode}_{world_size}gpu_{amp_tag}'\n",
    "\n",
    "    # Profile: 2 warmup steps, 3 active steps\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        schedule=schedule(wait=0, warmup=2, active=3, repeat=1),\n",
    "        on_trace_ready=lambda p: p.export_chrome_trace(f'{trace_name}_rank{rank}.json'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "    ) as prof:\n",
    "        for step in range(5):\n",
    "            xb = X_all[:bs]\n",
    "            yb = y_all[:bs]\n",
    "            chunk = len(xb) // world_size\n",
    "            x_local = xb[rank*chunk:(rank+1)*chunk]\n",
    "            y_local = yb[rank*chunk:(rank+1)*chunk]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(x_local)\n",
    "                loss = loss_fn(out, y_local)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if mode == 'dp_naive':\n",
    "                for param in (model.module.parameters() if hasattr(model, 'module') else model.parameters()):\n",
    "                    if param.grad is not None:\n",
    "                        dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n",
    "                        param.grad /= world_size\n",
    "            elif mode == 'dp_interleaved':\n",
    "                for h, p in handles_list:\n",
    "                    h.wait()\n",
    "                    p.grad /= world_size\n",
    "                handles_list.clear()\n",
    "            # DDP handles AllReduce automatically\n",
    "\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            prof.step()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f'TRACE_SAVED:{trace_name}_rank0.json')\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, json, torch\n\nNUM_GPUS = torch.cuda.device_count()\nng = min(NUM_GPUS, 4)  # Profile with up to 4 GPUs\n\nprint(f\"Profiling with {ng} GPUs...\")\nprint(\"Traces will be saved to ./profiles/\\n\")\n\nport = 29600\nfor mode in ['dp_naive', 'dp_interleaved', 'ddp_builtin']:\n    for use_amp in [False, True]:\n        amp_tag = 'AMP' if use_amp else 'FP32'\n        print(f\"  Profiling {mode} ({amp_tag})...\")\n        cfg = json.dumps({'mode': mode, 'model_size': 'large', 'use_amp': use_amp})\n        port += 1\n        cmd = ['torchrun', '--nproc_per_node', str(ng),\n               '--master_port', str(port), 'run_profiler.py', cfg]\n        r = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n        if r.returncode == 0:\n            for line in r.stdout.split('\\n'):\n                if line.startswith('TRACE_SAVED:'):\n                    print(f\"    Saved: {line.split(':',1)[1]}\")\n        else:\n            print(f\"    ERROR: {r.stderr[:200]}\")\n\nprint(\"\\n Profiling complete!\")\nprint(\"To view traces: open chrome://tracing and load the JSON files from ./profiles/\")\nprint(\"\\nWhat to look for:\")\nprint(\"  - dp_naive:       NCCL kernels appear AFTER compute (sequential, no overlap)\")\nprint(\"  - dp_interleaved: NCCL kernels OVERLAP with backward compute\")\nprint(\"  - ddp_builtin:    Similar overlap but fewer, larger NCCL calls (bucketing)\")\nprint(\"  - AMP vs FP32:    AMP traces show smaller NCCL transfers (half the data)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated trace files\n",
    "import glob\n",
    "traces = sorted(glob.glob('profiles/*.json'))\n",
    "print(f\"Generated {len(traces)} trace files:\\n\")\n",
    "for t in traces:\n",
    "    size_mb = os.path.getsize(t) / 1024 / 1024\n",
    "    print(f\"  {t:60s} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nDownload these and open at chrome://tracing to see GPU timelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json, numpy as np, matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 130\nmatplotlib.rcParams['font.size'] = 10\n\nwith open('benchmark_results.json') as f:\n    results = json.load(f)\n\nN_SAMPLES = 65536  # matches our config\nmodel_order = ['base', 'large', 'xlarge']\n\nprint(f\"Loaded {len(results)} results\")\nprint(f\"GPU: {results[0].get('gpu_name', 'unknown')}\")\nprint(f\"\\n{'Model':<8} {'Mode':<20} {'GPUs':>4} {'AMP':>4} {'GA':>3} {'Params':>10} {'Epoch':>9} {'Acc':>7}\")\nprint('─' * 75)\nfor r in results:\n    amp_str = 'Yes' if r.get('use_amp') else 'No'\n    ga = r.get('grad_accum_steps', 1)\n    print(f\"{r['model_size']:<8} {r['mode']:<20} {r['n_gpus']:>4} {amp_str:>4} {ga:>3} \"\n          f\"{r['n_params']/1e6:>8.1f}M {r['avg_epoch_time']:>8.4f}s {r['final_accuracy']:>6.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 1: FP32 vs AMP Epoch Time — Grouped by Model Size\n",
    "# Shows the speedup from mixed precision\n",
    "# ========================================================\n",
    "\n",
    "# Filter to grad_accum=1, DDP mode for clarity\n",
    "ddp_results = [r for r in results if r['mode'] == 'ddp_builtin' and r.get('grad_accum_steps', 1) == 1]\n",
    "\n",
    "model_order = ['small', 'medium', 'large', 'xlarge']\n",
    "model_sizes_present = sorted(set(r['model_size'] for r in ddp_results),\n",
    "                              key=lambda x: model_order.index(x) if x in model_order else 99)\n",
    "gpu_counts_present = sorted(set(r['n_gpus'] for r in ddp_results))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(model_sizes_present), figsize=(5*len(model_sizes_present), 5), sharey=False)\n",
    "if len(model_sizes_present) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, ms in enumerate(model_sizes_present):\n",
    "    ax = axes[idx]\n",
    "    x_labels, fp32_vals, amp_vals = [], [], []\n",
    "\n",
    "    for ng in gpu_counts_present:\n",
    "        fp32 = [r for r in ddp_results if r['model_size'] == ms and r['n_gpus'] == ng and not r.get('use_amp')]\n",
    "        amp = [r for r in ddp_results if r['model_size'] == ms and r['n_gpus'] == ng and r.get('use_amp')]\n",
    "        if fp32 and amp:\n",
    "            x_labels.append(f'{ng} GPU')\n",
    "            fp32_vals.append(fp32[0]['avg_epoch_time'])\n",
    "            amp_vals.append(amp[0]['avg_epoch_time'])\n",
    "\n",
    "    # Add single GPU\n",
    "    sg_fp32 = [r for r in results if r['model_size'] == ms and r['mode'] == 'single_gpu'\n",
    "               and not r.get('use_amp') and r.get('grad_accum_steps', 1) == 1]\n",
    "    sg_amp = [r for r in results if r['model_size'] == ms and r['mode'] == 'single_gpu'\n",
    "              and r.get('use_amp') and r.get('grad_accum_steps', 1) == 1]\n",
    "    if sg_fp32 and sg_amp:\n",
    "        x_labels.insert(0, '1 GPU')\n",
    "        fp32_vals.insert(0, sg_fp32[0]['avg_epoch_time'])\n",
    "        amp_vals.insert(0, sg_amp[0]['avg_epoch_time'])\n",
    "\n",
    "    x = np.arange(len(x_labels))\n",
    "    w = 0.35\n",
    "    ax.bar(x - w/2, fp32_vals, w, label='FP32', color='#FF7043', edgecolor='white')\n",
    "    ax.bar(x + w/2, amp_vals, w, label='AMP (FP16/BF16)', color='#42A5F5', edgecolor='white')\n",
    "\n",
    "    # Annotate speedup\n",
    "    for i in range(len(x_labels)):\n",
    "        if fp32_vals[i] > 0 and amp_vals[i] > 0:\n",
    "            speedup = fp32_vals[i] / amp_vals[i]\n",
    "            ax.text(x[i], max(fp32_vals[i], amp_vals[i]) * 1.05,\n",
    "                    f'{speedup:.2f}×', ha='center', fontsize=8, fontweight='bold', color='#1565C0')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_ylabel('Epoch Time (s)')\n",
    "    n_params = [r for r in results if r['model_size'] == ms][0]['n_params']\n",
    "    ax.set_title(f'{ms.capitalize()} ({n_params/1e6:.1f}M)', fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "fig.suptitle('FP32 vs Mixed Precision (AMP) — DDP Epoch Time', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_amp_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 2: Scaling Efficiency — 1 to 8 GPUs\n",
    "# FP32 vs AMP side by side\n",
    "# ========================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "for amp_idx, (use_amp, amp_label) in enumerate([(False, 'FP32'), (True, 'AMP')]):\n",
    "    ax = axes[amp_idx]\n",
    "    ga1_results = [r for r in results if r.get('use_amp') == use_amp and r.get('grad_accum_steps', 1) == 1]\n",
    "\n",
    "    msizes = sorted(set(r['model_size'] for r in ga1_results),\n",
    "                     key=lambda x: model_order.index(x) if x in model_order else 99)\n",
    "\n",
    "    colors_map = {'small': '#FF7043', 'medium': '#42A5F5', 'large': '#66BB6A', 'xlarge': '#AB47BC'}\n",
    "\n",
    "    for ms in msizes:\n",
    "        baseline = [r for r in ga1_results if r['model_size'] == ms and r['mode'] == 'single_gpu']\n",
    "        if not baseline:\n",
    "            continue\n",
    "        base_time = baseline[0]['avg_epoch_time']\n",
    "\n",
    "        gpus, speedups = [1], [1.0]\n",
    "        for ng in sorted(set(r['n_gpus'] for r in ga1_results if r['mode'] == 'ddp_builtin')):\n",
    "            m = [r for r in ga1_results if r['model_size'] == ms and r['mode'] == 'ddp_builtin' and r['n_gpus'] == ng]\n",
    "            if m:\n",
    "                gpus.append(ng)\n",
    "                speedups.append(base_time / m[0]['avg_epoch_time'])\n",
    "\n",
    "        n_params = baseline[0]['n_params']\n",
    "        ax.plot(gpus, speedups, '-o', color=colors_map.get(ms, 'gray'),\n",
    "                label=f'{ms} ({n_params/1e6:.0f}M)', linewidth=2, markersize=7)\n",
    "\n",
    "    max_g = max(r['n_gpus'] for r in ga1_results) if ga1_results else 8\n",
    "    ax.plot([1, max_g], [1, max_g], ':k', alpha=0.3, label='Ideal linear')\n",
    "    ax.set_xlabel('Number of GPUs')\n",
    "    ax.set_ylabel('Speedup vs 1 GPU')\n",
    "    ax.set_title(f'{amp_label} — DDP Scaling', fontweight='bold')\n",
    "    ax.set_xticks([1, 2, 4, 8][:len(set(r['n_gpus'] for r in ga1_results))+1])\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "fig.suptitle('GPU Scaling: FP32 vs AMP — PyTorch DDP', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_scaling_fp32_vs_amp.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 3: Gradient Accumulation Effect\n",
    "# GA=1 vs GA=4 — fewer AllReduce calls per epoch\n",
    "# ========================================================\n",
    "\n",
    "# Use DDP + AMP for this comparison\n",
    "ga_results = [r for r in results if r['mode'] == 'ddp_builtin' and r.get('use_amp', False)]\n",
    "\n",
    "if len(set(r.get('grad_accum_steps', 1) for r in ga_results)) > 1:\n",
    "    msizes = sorted(set(r['model_size'] for r in ga_results),\n",
    "                     key=lambda x: model_order.index(x) if x in model_order else 99)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(msizes), figsize=(5*len(msizes), 5), sharey=False)\n",
    "    if len(msizes) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, ms in enumerate(msizes):\n",
    "        ax = axes[idx]\n",
    "        for ga, color, style in [(1, '#FF7043', '-o'), (4, '#42A5F5', '-s')]:\n",
    "            subset = [r for r in ga_results if r['model_size'] == ms and r.get('grad_accum_steps', 1) == ga]\n",
    "            if not subset:\n",
    "                continue\n",
    "            gpus = sorted(set(r['n_gpus'] for r in subset))\n",
    "            times = [next(r['avg_epoch_time'] for r in subset if r['n_gpus'] == ng) for ng in gpus]\n",
    "            ax.plot(gpus, times, style, color=color, label=f'GA={ga}', linewidth=2, markersize=7)\n",
    "\n",
    "        # Also add single GPU\n",
    "        for ga, color, style in [(1, '#FF7043', '-o'), (4, '#42A5F5', '-s')]:\n",
    "            sg = [r for r in results if r['model_size'] == ms and r['mode'] == 'single_gpu'\n",
    "                  and r.get('use_amp', False) and r.get('grad_accum_steps', 1) == ga]\n",
    "            if sg:\n",
    "                ax.plot(1, sg[0]['avg_epoch_time'], 'D', color=color, markersize=8)\n",
    "\n",
    "        ax.set_xlabel('Number of GPUs')\n",
    "        ax.set_ylabel('Epoch Time (s)')\n",
    "        n_params = [r for r in results if r['model_size'] == ms][0]['n_params']\n",
    "        ax.set_title(f'{ms.capitalize()} ({n_params/1e6:.1f}M)', fontweight='bold')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xticks([1, 2, 4, 8][:len(set(r['n_gpus'] for r in ga_results))+1])\n",
    "\n",
    "    fig.suptitle('Gradient Accumulation: GA=1 vs GA=4 (DDP + AMP)\\n'\n",
    "                 'GA=4 means 4× fewer AllReduce calls per epoch',\n",
    "                 fontsize=13, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('chart_grad_accum.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Only one grad_accum setting found — skipping GA comparison chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 4: Communication Overhead — Naive DP\n",
    "# FP32 vs AMP: AMP should show less comm time (half the gradients)\n",
    "# ========================================================\n",
    "\n",
    "naive_results = [r for r in results if r['mode'] == 'dp_naive' and 'comm_times' in r\n",
    "                 and r.get('grad_accum_steps', 1) == 1]\n",
    "\n",
    "if naive_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for amp_idx, (use_amp, amp_label) in enumerate([(False, 'FP32'), (True, 'AMP')]):\n",
    "        ax = axes[amp_idx]\n",
    "        subset = [r for r in naive_results if r.get('use_amp') == use_amp]\n",
    "        if not subset:\n",
    "            ax.text(0.5, 0.5, f'No {amp_label} naive results', ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "\n",
    "        labels, comp, comm = [], [], []\n",
    "        for r in sorted(subset, key=lambda x: (model_order.index(x['model_size'])\n",
    "                         if x['model_size'] in model_order else 99, x['n_gpus'])):\n",
    "            labels.append(f\"{r['model_size']}\\n{r['n_gpus']}G\")\n",
    "            avg_comm = r.get('avg_comm_time', 0)\n",
    "            avg_total = r['avg_epoch_time']\n",
    "            comp.append(avg_total - avg_comm)\n",
    "            comm.append(avg_comm)\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        ax.bar(x, comp, 0.6, label='Computation', color='#42A5F5')\n",
    "        ax.bar(x, comm, 0.6, bottom=comp, label='Communication', color='#FF7043')\n",
    "\n",
    "        for i, (c, cm) in enumerate(zip(comp, comm)):\n",
    "            total = c + cm\n",
    "            pct = cm / total * 100 if total > 0 else 0\n",
    "            ax.text(i, total + 0.01, f'{pct:.0f}%', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel('Time (s)')\n",
    "        ax.set_title(f'Naive DP — {amp_label}', fontweight='bold')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    fig.suptitle('Communication Overhead: FP32 vs AMP\\n'\n",
    "                 'AMP halves gradient size → less AllReduce data → lower comm %',\n",
    "                 fontsize=12, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('chart_comm_fp32_vs_amp.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 5: Loss Convergence — Fixed Global Average\n",
    "# All modes should now overlap perfectly within same config\n",
    "# ========================================================\n",
    "\n",
    "# Pick largest model, AMP, GA=1\n",
    "all_msizes = sorted(set(r['model_size'] for r in results),\n",
    "                     key=lambda x: model_order.index(x) if x in model_order else 99)\n",
    "target_model = all_msizes[-1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "style_map = {\n",
    "    'single_gpu': ('-', '#37474F', 3),\n",
    "    'dp_naive': ('--', '#FF5722', 2),\n",
    "    'dp_interleaved': ('-', '#1E88E5', 2),\n",
    "    'ddp_builtin': ('-.', '#2E7D32', 2),\n",
    "}\n",
    "\n",
    "for amp_idx, (use_amp, amp_label) in enumerate([(False, 'FP32'), (True, 'AMP')]):\n",
    "    ax = axes[amp_idx]\n",
    "    subset = [r for r in results if r['model_size'] == target_model\n",
    "              and r.get('use_amp') == use_amp and r.get('grad_accum_steps', 1) == 1\n",
    "              and 'losses' in r]\n",
    "\n",
    "    for r in subset:\n",
    "        ls, c, lw = style_map.get(r['mode'], ('-', 'gray', 1))\n",
    "        lbl = f\"{r['mode']} ({r['n_gpus']}G)\"\n",
    "        ax.plot(range(1, len(r['losses'])+1), r['losses'], linestyle=ls, color=c,\n",
    "                linewidth=lw, marker='o', markersize=4, label=lbl)\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Training Loss (global average)')\n",
    "    ax.set_title(f'{amp_label} — {target_model.capitalize()} Model', fontweight='bold')\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "fig.suptitle('Loss Convergence — Fixed Global Average Logging\\n'\n",
    "             'All modes within same GPU count should now overlap (same math, same loss)',\n",
    "             fontsize=12, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_loss_convergence_fixed.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CHART 6: Naive vs Interleaved vs DDP — All GPU Counts\n",
    "# ========================================================\n",
    "\n",
    "# Use AMP + GA=1 for this\n",
    "compare_results = [r for r in results if r.get('use_amp', False) and r.get('grad_accum_steps', 1) == 1]\n",
    "\n",
    "msizes = sorted(set(r['model_size'] for r in compare_results),\n",
    "                 key=lambda x: model_order.index(x) if x in model_order else 99)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(14, len(msizes)*5), 6))\n",
    "pair_labels, naive_t, interl_t, ddp_t = [], [], [], []\n",
    "\n",
    "for ms in msizes:\n",
    "    for ng in sorted(set(r['n_gpus'] for r in compare_results if r['n_gpus'] > 1)):\n",
    "        n = [r for r in compare_results if r['model_size'] == ms and r['mode'] == 'dp_naive' and r['n_gpus'] == ng]\n",
    "        il = [r for r in compare_results if r['model_size'] == ms and r['mode'] == 'dp_interleaved' and r['n_gpus'] == ng]\n",
    "        dd = [r for r in compare_results if r['model_size'] == ms and r['mode'] == 'ddp_builtin' and r['n_gpus'] == ng]\n",
    "        if n and il and dd:\n",
    "            pair_labels.append(f\"{ms}\\n{ng}G\")\n",
    "            naive_t.append(n[0]['avg_epoch_time'])\n",
    "            interl_t.append(il[0]['avg_epoch_time'])\n",
    "            ddp_t.append(dd[0]['avg_epoch_time'])\n",
    "\n",
    "x = np.arange(len(pair_labels))\n",
    "w = 0.25\n",
    "ax.bar(x - w, naive_t, w, label='Naive DP', color='#FF7043', edgecolor='white')\n",
    "ax.bar(x, interl_t, w, label='Interleaved DP', color='#42A5F5', edgecolor='white')\n",
    "ax.bar(x + w, ddp_t, w, label='PyTorch DDP', color='#66BB6A', edgecolor='white')\n",
    "\n",
    "for i in range(len(pair_labels)):\n",
    "    if naive_t[i] > 0 and interl_t[i] > 0:\n",
    "        saving = (1 - interl_t[i] / naive_t[i]) * 100\n",
    "        y = max(naive_t[i], interl_t[i], ddp_t[i]) + 0.01\n",
    "        if saving > 0:\n",
    "            ax.annotate(f'{saving:.0f}% faster', xy=(i, y), fontsize=7,\n",
    "                        ha='center', fontweight='bold', color='#1565C0')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pair_labels)\n",
    "ax.set_ylabel('Epoch Time (s)')\n",
    "ax.set_title('Naive vs Interleaved vs DDP (AMP) — All GPU Counts', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_mode_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================================\n# CHART 7: Throughput (samples/sec) — The Real Metric\n# ========================================================\n\n# Use AMP + GA=1 for throughput comparison\ntp_results = [r for r in results if r.get('use_amp', False) and r.get('grad_accum_steps', 1) == 1]\n\n# Collect unique combos\ncombos = []\nseen = set()\nfor r in tp_results:\n    key = (r['mode'], r['n_gpus'])\n    if key not in seen:\n        seen.add(key)\n        combos.append(key)\n\ncolors = {\n    ('single_gpu', 1): '#37474F',\n    ('dp_naive', 2): '#FF7043', ('dp_naive', 4): '#FF5722', ('dp_naive', 8): '#D84315',\n    ('dp_interleaved', 2): '#42A5F5', ('dp_interleaved', 4): '#1E88E5', ('dp_interleaved', 8): '#1565C0',\n    ('ddp_builtin', 2): '#66BB6A', ('ddp_builtin', 4): '#43A047', ('ddp_builtin', 8): '#2E7D32',\n}\n\ndef combo_label(mode, ng):\n    names = {'single_gpu': '1G', 'dp_naive': f'Naive {ng}G',\n             'dp_interleaved': f'Interl {ng}G', 'ddp_builtin': f'DDP {ng}G'}\n    return names.get(mode, f'{mode} ({ng}G)')\n\nmsizes = sorted(set(r['model_size'] for r in tp_results),\n                 key=lambda x: model_order.index(x) if x in model_order else 99)\n\nfig, ax = plt.subplots(figsize=(max(14, len(msizes)*4), 6))\ntp_data = {}\nfor r in tp_results:\n    key = (r['model_size'], r['mode'], r['n_gpus'])\n    tp_data[key] = N_SAMPLES / r['avg_epoch_time']\n\nfor ms_idx, ms in enumerate(msizes):\n    tp_vals, tp_colors_list = [], []\n    for mode, ng in combos:\n        key = (ms, mode, ng)\n        if key in tp_data:\n            tp_vals.append(tp_data[key])\n            tp_colors_list.append(colors.get((mode, ng), f'C{len(tp_vals)}'))\n\n    x = np.arange(len(tp_vals))\n    offset = ms_idx * (len(tp_vals) + 1.5)\n    bars = ax.bar(x + offset, tp_vals, 0.8, color=tp_colors_list, edgecolor='white')\n    n_params = [r for r in results if r['model_size'] == ms][0]['n_params']\n    ax.text(offset + len(tp_vals)/2 - 0.5, -max(tp_data.values())*0.08,\n            f'{ms.capitalize()}\\n({n_params/1e6:.0f}M)',\n            ha='center', fontweight='bold', fontsize=10)\n    for bar, v in zip(bars, tp_vals):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(tp_data.values())*0.01,\n                f'{v:.0f}', ha='center', fontsize=6, fontweight='bold', rotation=90)\n\nax.set_ylabel('Throughput (samples/sec)')\nax.set_title('Training Throughput (AMP) — All GPU Counts', fontweight='bold')\nax.set_xticks([])\n\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=colors.get((m,g), 'gray'), label=combo_label(m,g))\n                   for m,g in combos if (m,g) in colors]\nax.legend(handles=legend_elements, fontsize=7, ncol=3)\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig('chart_throughput_amp.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"GPU: {results[0].get('gpu_name', 'unknown')}\")\nprint(f\"\\n{'Model':<8} {'Mode':<20} {'GPUs':>4} {'AMP':>4} {'GA':>3} {'Params':>10} {'Epoch':>9} \"\n      f\"{'Comm':>8} {'Thruput':>10} {'Speedup':>8} {'Acc':>7}\")\nprint('─' * 105)\n\nfor ms in sorted(set(r['model_size'] for r in results),\n                  key=lambda x: model_order.index(x) if x in model_order else 99):\n    # Baseline: single GPU FP32 GA=1\n    baseline = [r for r in results if r['model_size'] == ms and r['mode'] == 'single_gpu'\n                and not r.get('use_amp') and r.get('grad_accum_steps', 1) == 1]\n    base_time = baseline[0]['avg_epoch_time'] if baseline else 1.0\n\n    subset = sorted([r for r in results if r['model_size'] == ms],\n                     key=lambda x: (x.get('use_amp', False), x.get('grad_accum_steps', 1),\n                                    x['n_gpus'], x['mode']))\n    for r in subset:\n        throughput = N_SAMPLES / r['avg_epoch_time']\n        speedup = base_time / r['avg_epoch_time']\n        comm = r.get('avg_comm_time', r.get('comm_time', 0))\n        if isinstance(comm, list):\n            comm = sum(comm) / len(comm)\n        amp_str = 'Yes' if r.get('use_amp') else 'No'\n        ga = r.get('grad_accum_steps', 1)\n        print(f\"{r['model_size']:<8} {r['mode']:<20} {r['n_gpus']:>4} {amp_str:>4} {ga:>3} \"\n              f\"{r['n_params']/1e6:>8.1f}M {r['avg_epoch_time']:>8.4f}s \"\n              f\"{comm:>7.4f}s {throughput:>8.0f}/s {speedup:>7.2f}x {r['final_accuracy']:>6.4f}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Understanding the New Features\n",
    "\n",
    "### Mixed Precision (AMP)\n",
    "\n",
    "When you enable `torch.amp.autocast`, the forward pass runs in FP16/BF16 instead of FP32:\n",
    "- **Compute benefit**: FP16 matrix multiplications are 2× faster on Tensor Cores (A100/H100)\n",
    "- **Communication benefit**: Gradients are half the size → AllReduce transfers half the data\n",
    "- **Memory benefit**: Activations stored in FP16 → can fit larger batches\n",
    "\n",
    "The `GradScaler` prevents underflow by dynamically scaling the loss before backward (only needed for FP16, not BF16).\n",
    "\n",
    "### Gradient Accumulation\n",
    "\n",
    "Instead of syncing gradients every micro-batch:\n",
    "```\n",
    "GA=1: [FWD+BWD → AllReduce → Step] [FWD+BWD → AllReduce → Step] ...\n",
    "GA=4: [FWD+BWD] [FWD+BWD] [FWD+BWD] [FWD+BWD → AllReduce → Step]\n",
    "```\n",
    "\n",
    "With GA=4, you do 4× fewer AllReduce calls per epoch. The gradient from 4 micro-batches accumulates locally, then one AllReduce syncs everything. This:\n",
    "- **Reduces communication overhead** (fewer AllReduce calls)\n",
    "- **Simulates larger batch size** (effective_batch = batch_size × GA × n_gpus)\n",
    "- **Uses DDP's `no_sync()`** context manager to skip AllReduce during accumulation steps\n",
    "\n",
    "### torch.profiler Traces\n",
    "\n",
    "The profiler captures actual GPU kernel timelines. When you open the traces in `chrome://tracing`:\n",
    "- **Compute kernels** (gemm, relu, etc.) appear on the CUDA stream rows\n",
    "- **NCCL kernels** (AllReduce) appear on separate NCCL stream rows\n",
    "- **Naive DP**: NCCL blocks are sequential after compute — you see a gap\n",
    "- **Interleaved/DDP**: NCCL blocks overlap with compute — the streams run in parallel\n",
    "\n",
    "### Fixed Loss Logging\n",
    "\n",
    "The original notebook logged `loss.item()` from rank 0's local micro-batch, creating artificial separation between GPU-count clusters. Now we do:\n",
    "```python\n",
    "loss_val = loss.detach()\n",
    "dist.all_reduce(loss_val, op=dist.ReduceOp.SUM)  # sync across ranks\n",
    "global_loss = loss_val.item() / world_size         # true average\n",
    "```\n",
    "All modes within the same config should now produce identical loss curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. NVLink vs PCIe Comparison Guide\n",
    "\n",
    "To compare interconnects:\n",
    "\n",
    "1. **Run this notebook on a PCIe machine** (e.g., RunPod `4x A100 80GB PCIe`)\n",
    "   - Results auto-save to `benchmark_results_NVIDIA_A100_80GB_PCIe.json`\n",
    "\n",
    "2. **Run again on an NVLink/SXM machine** (e.g., RunPod `4x A100 80GB SXM`)\n",
    "   - Results save to `benchmark_results_NVIDIA_A100_80GB_SXM.json`\n",
    "\n",
    "3. **Compare** using the cell below (upload both JSON files):\n",
    "\n",
    "### What to expect:\n",
    "| Metric | PCIe | NVLink |\n",
    "|--------|------|--------|\n",
    "| AllReduce bandwidth | ~32 GB/s | ~600 GB/s |\n",
    "| Comm overhead (naive) | **High** | Low |\n",
    "| Interleaved benefit | **Large** (more to hide) | Smaller (less to hide) |\n",
    "| Scaling efficiency | Drops at 8 GPUs | Near-linear at 8 GPUs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================================\n# Optional: Load two result files and compare\n# Upload your PCIe and NVLink JSON files, then run this\n# ========================================================\n\nimport glob\n\nresult_files = sorted(glob.glob('benchmark_results_*.json'))\nprint(f\"Found {len(result_files)} result files:\")\nfor f in result_files:\n    print(f\"  {f}\")\n\nif len(result_files) >= 2:\n    all_hw_results = {}\n    for rf in result_files:\n        with open(rf) as f:\n            data = json.load(f)\n        gpu_name = data[0].get('gpu_name', rf)\n        all_hw_results[gpu_name] = data\n\n    # Compare DDP AMP GA=1 across hardware\n    fig, ax = plt.subplots(figsize=(14, 6))\n    hw_names = list(all_hw_results.keys())\n    hw_colors = ['#FF7043', '#42A5F5', '#66BB6A', '#AB47BC']\n\n    bar_groups = []\n    for ms in model_order:\n        for ng in [2, 4, 8]:\n            has_data = False\n            for hw in hw_names:\n                match = [r for r in all_hw_results[hw]\n                         if r['model_size'] == ms and r['mode'] == 'ddp_builtin'\n                         and r['n_gpus'] == ng and r.get('use_amp') and r.get('grad_accum_steps', 1) == 1]\n                if match:\n                    has_data = True\n            if has_data:\n                bar_groups.append((ms, ng))\n\n    x = np.arange(len(bar_groups))\n    w = 0.8 / len(hw_names)\n\n    for hw_idx, hw in enumerate(hw_names):\n        vals = []\n        for ms, ng in bar_groups:\n            match = [r for r in all_hw_results[hw]\n                     if r['model_size'] == ms and r['mode'] == 'ddp_builtin'\n                     and r['n_gpus'] == ng and r.get('use_amp') and r.get('grad_accum_steps', 1) == 1]\n            vals.append(match[0]['avg_epoch_time'] if match else 0)\n        offset = (hw_idx - len(hw_names)/2 + 0.5) * w\n        ax.bar(x + offset, vals, w, label=hw, color=hw_colors[hw_idx % len(hw_colors)], edgecolor='white')\n\n    ax.set_xticks(x)\n    ax.set_xticklabels([f\"{ms}\\n{ng}G\" for ms, ng in bar_groups], fontsize=8)\n    ax.set_ylabel('Epoch Time (s)')\n    ax.set_title('Hardware Comparison — DDP + AMP', fontweight='bold')\n    ax.legend(fontsize=8)\n    ax.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('chart_hardware_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"\\nUpload a second benchmark_results_*.json file to enable comparison.\")\n    print(\"Run this notebook on different hardware and copy the result file here.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. RunPod Setup Guide\n",
    "\n",
    "### Recommended configurations:\n",
    "\n",
    "| Config | GPUs | Interconnect | What you'll learn |\n",
    "|--------|------|-------------|------------------|\n",
    "| **4x A100 80GB PCIe** | 4 | PCIe 4.0 (~32 GB/s) | Baseline scaling, comm overhead is visible |\n",
    "| **4x A100 80GB SXM** | 4 | NVLink 3.0 (~600 GB/s) | NVLink vs PCIe comparison |\n",
    "| **8x A100 80GB SXM** | 8 | NVLink 3.0 | Full 8-GPU scaling, diminishing returns |\n",
    "| **8x H100 80GB SXM** | 8 | NVLink 4.0 (~900 GB/s) | State-of-the-art scaling |\n",
    "| **4x RTX 4090** | 4 | PCIe 4.0 | Consumer GPU, high comm overhead |\n",
    "\n",
    "### Steps:\n",
    "1. Create a RunPod instance with your chosen GPU config\n",
    "2. Upload this notebook\n",
    "3. Run all cells\n",
    "4. Download `benchmark_results_<GPU_NAME>.json`\n",
    "5. Repeat on different hardware\n",
    "6. Upload all JSON files to the comparison cell above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}